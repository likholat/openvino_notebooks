{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion v1.5 using OpenVINO `TorchDynamo` backend\n",
    "\n",
    "Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/). It is trained on 512x512 images from a subset of the [LAION-5B](https://laion.ai/blog/laion-5b/) database. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder. See the [model card](https://huggingface.co/CompVis/stable-diffusion) for more information.\n",
    "\n",
    "This notebook demonstrates how to run stable diffusion model using [Diffusers](https://huggingface.co/docs/diffusers/index) library and [OpenVINO `TorchDynamo` backend](https://docs.openvino.ai/2023.1/pytorch_2_0_torch_compile.html) for Text-to-Image and Image-to-Image generation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openvino\n",
    "%pip install torch==2.0.1\n",
    "%pip install diffusers==0.17.1\n",
    "%pip install transformers\n",
    "%pip install gradio==3.36.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can export `OV_DEVICE` environmental variable to choose the inference device between \"CPU\", \"GPU\" or \"GPU.0\" mean Intel integrated GPU, \"GPU.1\" â€“ Intel discrete GPU. If the system does not have an integrated GPU, use \"GPU.0\" for Intel discrete GPU. Read more about [Device Naming Convention](https://docs.openvino.ai/2023.0/openvino_docs_OV_UG_supported_plugins_GPU.html).\n",
    "\n",
    "`OPENVINO_TORCH_MODEL_CACHING` variable enables saving the optimized model files to a hard drive, after the first application run. This makes them available for the following application executions, reducing the first-inference latency.\n",
    "\n",
    "Read more about available [Environment Variables options](https://docs.openvino.ai/2023.1/pytorch_2_0_torch_compile.html#environment-variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OPENVINO_TORCH_BACKEND_DEVICE=\"CPU\"\n",
    "%env OPENVINO_TORCH_MODEL_CACHING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "import torch\n",
    "import time\n",
    "import openvino.torch # noqa: F401\n",
    "\n",
    "from socket import gethostbyname, gethostname\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Diffusion with Diffusers library\n",
    "\n",
    "To work with Stable Diffusion v1, we will use Hugging Face Diffusers library. To experiment with Stable Diffusion models, Diffusers exposes the [StableDiffusionPipeline](https://huggingface.co/docs/diffusers/using-diffusers/conditional_image_generation) and [StableDiffusionImg2ImgPipeline](https://huggingface.co/docs/diffusers/using-diffusers/img2img) similar to the other [Diffusers pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview). The code below demonstrates how to create the `StableDiffusionPipeline` and `StableDiffusionImg2ImgPipeline` using `stable-diffusion-1-5` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "# Pipeline for text-to-image generation\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float32)\n",
    "\n",
    "# Pipeline for text-guided image-to-image generation\n",
    "pipe_i2i = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [OpenVINO TorchDynamo backend]() lets you enable [OpenVINO](https://docs.openvino.ai/2023.0/home.html) support for PyTorch models with minimal changes to the original PyTorch script.\n",
    "\n",
    "Now we can enable the OpenVINO optimization just with [`torch.compile()` method](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.unet = torch.compile(pipe.unet, backend=\"openvino\")\n",
    "pipe_i2i.unet = torch.compile(pipe_i2i.unet, backend=\"openvino\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the inference methods for text-to-image and image-to-image generation using Diffusers pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_stamps = []\n",
    "\n",
    "\n",
    "def callback(iter, t, latents):\n",
    "    time_stamps.append(time.time())\n",
    "\n",
    "\n",
    "def txt_to_img(prompt, neg_prompt, guidance, steps, width, height, generator):\n",
    "    return pipe(\n",
    "        prompt,\n",
    "        negative_prompt=neg_prompt,\n",
    "        num_inference_steps=int(steps),\n",
    "        guidance_scale=guidance,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        generator=generator,\n",
    "        callback=callback,\n",
    "        callback_steps=1).images\n",
    "\n",
    "\n",
    "def img_to_img(prompt, neg_prompt, img, strength, guidance, steps, generator):\n",
    "    img = img['image']\n",
    "    return pipe_i2i(\n",
    "        prompt,\n",
    "        negative_prompt=neg_prompt,\n",
    "        image=img,\n",
    "        num_inference_steps=int(steps),\n",
    "        strength=strength,\n",
    "        guidance_scale=guidance,\n",
    "        generator=generator,\n",
    "        callback=callback,\n",
    "        callback_steps=1).images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Text-to-Image or Image-to-Image generation\n",
    "Now you can start the demo, choose the inference mode, define prompts (and input image for Image-to-Image generation) and run inference pipeline.\n",
    "Optionally, you can also change some input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_str(error, title=\"Error\"):\n",
    "    return f\"\"\"#### {title}\n",
    "            {error}\"\"\" if error else \"\"\n",
    "\n",
    "\n",
    "def on_mode_change(mode):\n",
    "    return gr.update(visible=mode == modes['img2img']), \\\n",
    "        gr.update(visible=mode == modes['txt2img'])\n",
    "\n",
    "\n",
    "def inference(inf_mode, prompt, guidance=7.5, steps=25, width=768, height=768, seed=-1, img=None, strength=0.5, neg_prompt=\"\"):\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, 10000000)\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    res = None\n",
    "\n",
    "    global time_stamps\n",
    "    time_stamps = []\n",
    "    try:\n",
    "        if inf_mode == modes['txt2img']:\n",
    "            res = txt_to_img(prompt, neg_prompt, guidance, steps, width, height, generator)\n",
    "        elif inf_mode == modes['img2img']:\n",
    "            if img is None:\n",
    "                return None, None, gr.update(visible=True, value=error_str(\"Image is required for Image to Image mode\"))\n",
    "            res = img_to_img(prompt, neg_prompt, img, strength, guidance, steps, generator)\n",
    "    except Exception as e:\n",
    "        return None, None, gr.update(visible=True, value=error_str(e))\n",
    "    \n",
    "    warmup_duration = time_stamps[1] - time_stamps[0]\n",
    "    generation_rate = (steps - 1) / (time_stamps[-1] - time_stamps[1])\n",
    "    res_info = \"Warm up time: \" + str(round(warmup_duration, 2)) + \" secs \"\n",
    "    if (generation_rate >= 1.0):\n",
    "        res_info = res_info + \", Performance: \" + str(round(generation_rate, 2)) + \" it/s \"\n",
    "    else:\n",
    "        res_info = res_info + \", Performance: \" + str(round(1 / generation_rate, 2)) + \" s/it \"\n",
    "\n",
    "    return res, gr.update(visible=True, value=res_info), gr.update(visible=False, value=None)\n",
    "\n",
    "\n",
    "modes = {\n",
    "    'txt2img': 'Text to Image',\n",
    "    'img2img': 'Image to Image',\n",
    "}\n",
    "\n",
    "with gr.Blocks(css=\"style.css\") as demo:\n",
    "    gr.HTML(\n",
    "        f\"\"\"\n",
    "            Model used: {model_id}         \n",
    "        \"\"\"\n",
    "    )\n",
    "    with gr.Row():\n",
    "\n",
    "        with gr.Column(scale=60):\n",
    "            with gr.Group():\n",
    "                prompt = gr.Textbox(\"a photograph of an astronaut riding a horse\", label=\"Prompt\", max_lines=2)\n",
    "                neg_prompt = gr.Textbox(\"frames, borderline, text, character, duplicate, error, out of frame, watermark, low quality, ugly, deformed, blur\", label=\"Negative prompt\")\n",
    "                res_img = gr.Gallery(label=\"Generated images\", show_label=False)\n",
    "            error_output = gr.Markdown(visible=False)\n",
    "\n",
    "        with gr.Column(scale=40):\n",
    "            generate = gr.Button(value=\"Generate\")\n",
    "\n",
    "            with gr.Group():\n",
    "                inf_mode = gr.Dropdown(list(modes.values()), label=\"Inference Mode\", value=modes['txt2img'])\n",
    "                \n",
    "                with gr.Column(visible=False) as i2i:\n",
    "                    image = gr.Image(label=\"Image\", height=128, type=\"pil\", tool='sketch')\n",
    "                    strength = gr.Slider(label=\"Transformation strength\", minimum=0, maximum=1, step=0.01, value=0.5)\n",
    "\n",
    "            with gr.Group():\n",
    "                with gr.Row() as txt2i:\n",
    "                    width = gr.Slider(label=\"Width\", value=512, minimum=64, maximum=1024, step=8)\n",
    "                    height = gr.Slider(label=\"Height\", value=512, minimum=64, maximum=1024, step=8)\n",
    "\n",
    "            with gr.Group():\n",
    "                with gr.Row():\n",
    "                    steps = gr.Slider(label=\"Steps\", value=20, minimum=1, maximum=50, step=1)\n",
    "                    guidance = gr.Slider(label=\"Guidance scale\", value=7.5, maximum=15)\n",
    "\n",
    "                seed = gr.Slider(-1, 10000000, label='Seed (-1 = random)', value=-1, step=1)\n",
    "            \n",
    "            res_info = gr.Markdown(visible=False)\n",
    "\n",
    "    inf_mode.change(on_mode_change, inputs=[inf_mode], outputs=[\n",
    "                    i2i, txt2i], queue=False)\n",
    "\n",
    "    inputs = [inf_mode, prompt, guidance, steps,\n",
    "              width, height, seed, image, strength, neg_prompt]\n",
    "    \n",
    "    outputs = [res_img, res_info, error_output]\n",
    "    prompt.submit(inference, inputs=inputs, outputs=outputs)\n",
    "    generate.click(inference, inputs=inputs, outputs=outputs)\n",
    "\n",
    "ipaddr = gethostbyname(gethostname())\n",
    "demo.queue().launch(debug=True, server_name=ipaddr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ov_23.1_env",
   "language": "python",
   "name": "ov_23.1_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
